{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escape Game\n",
    "\n",
    "Authors : Yana RAGOZINA - Thomas PAUL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we seek to implement an intelligent agent for an escape game task.\n",
    "\n",
    "The agent must learn to find the most optimal way to reach room 5 starting from room 2, in a 6-room building connected by doors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rooms in the buiding can be represented as states. The agent can move from one room to another following the doors available in each room.\n",
    "Therefore, the agent can have 6 actions (= 6 possible rooms). Every movement between the rooms is followed by a reward, 100 being attributed to the final state, room 5. Other movements gain 0 reward, and -1 is assigned to impossible transitions (no door available between state n and n+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "states = 6\n",
    "actions = 6\n",
    "gamma = 0.8\n",
    "episodes = 100\n",
    "\n",
    "R = np.array([\n",
    "    [ -1, -1, -1, -1, 0, -1 ],\n",
    "    [ -1, -1, -1, 0, -1, 100 ],\n",
    "    [ -1, -1, -1, 0, -1, -1 ],\n",
    "    [ -1, 0, 0, -1, 0, -1 ],\n",
    "    [ 0, -1, -1, 0, -1, 100 ],\n",
    "    [ -1, 0, -1, -1, 0, 100 ],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the Q-table as follows: \n",
    "\n",
    "(-1 is assigned to impossible transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the Q-table\n",
    "Q = np.array([\n",
    "    [ -1, -1, -1, -1, 0, -1 ], # state (room) 0\n",
    "    [ -1, -1, -1, 0, -1, 0 ],  # state 1\n",
    "    [ -1, -1, -1, 0, -1, -1 ], # state 2\n",
    "    [ -1, 0, 0, -1, 0, -1 ],   # state 3\n",
    "    [ 0, -1, -1, 0, -1, 0 ],   # state 4\n",
    "    [ -1, 0, -1, -1, 0, 0 ],   # state 5\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1   0]\n",
      " [ -1   0  -1  -1   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# 2.a Using Bellman equation, compute the new value of Q(1,5)\n",
    "# 2.b Update the Q-table accordingly\n",
    "Q[1, 5] = R[1, 5] + gamma * np.max(Q[5,:])\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent receives the maximum reward, 100, reaching the final state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[3, 1]\n",
      " [[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1  80   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1   0]\n",
      " [ -1   0  -1  -1   0   0]]\n",
      "Q[1, 5]\n",
      " [[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1  80   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1   0]\n",
      " [ -1   0  -1  -1   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# 3. For the next episode, we start with a randomly chosen initial state. \n",
    "# This time, we have state 3 as our initial state and by random selection,  \n",
    "# we select to go to state 1 as our action. Update the Q(3,1) value\n",
    "Q[3, 1] = R[3, 1] + gamma * np.max(Q[1,:])\n",
    "\n",
    "print(f\"Q[3, 1]\\n {Q}\")\n",
    "\n",
    "#The next state, 1, now becomes the current state.  We repeat the inner loop of the Q \n",
    "#learning algorithm because state 1 is not the goal state. Assume we select again randomly \n",
    "#state 5. What is the new updated Q-table at the end of this episode ?\n",
    "Q[1, 5] = R[1, 5] + gamma * np.max(Q[5,:])\n",
    "\n",
    "print(f\"Q[1, 5]\\n {Q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1  -1  -1  -1  80  -1]\n",
      " [ -1  -1  -1  64  -1 100]\n",
      " [ -1  -1  -1  64  -1  -1]\n",
      " [ -1  80  51  -1  80  -1]\n",
      " [ 64  -1  -1  64  -1 100]\n",
      " [ -1   0  -1  -1   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# 4. Try to explore more episodes to find the Q-table reaching the convergence values.\n",
    "for episode in range(episodes):\n",
    "\n",
    "    state = random.randint(0, 5)\n",
    "\n",
    "    while state != 5:\n",
    "\n",
    "        target_action = random.randint(0, 5)\n",
    "\n",
    "        while R[state, target_action] == -1:\n",
    "            target_action = random.randint(0, 5)\n",
    "\n",
    "        Q[state, target_action] = R[state, target_action] + gamma * np.max(Q[target_action,:])\n",
    "\n",
    "        state = target_action\n",
    "\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-table starts to converge, highligting the most and the least probable transitions from state n to state n+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is normal that the Q[5, 5] is equal to 0 because we exit the loop once we have reached this state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe the probabilities of the agent's movements in each state. For example, in state 3 the agent will be most likely to move either to room 1 or room 4 (having equal probabilities) and will be the least likely to move to state 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "# Once the matrix Q gets close enough to a state of convergence, we know our agent has \n",
    "# learned the most optimal paths to the goal state.  Tracing the best sequences of states is as \n",
    "# simple as following the links with the highest values at each state. \n",
    "# What is the best sequence to escape the building from room 2 ?\n",
    "\n",
    "state = 2\n",
    "steps = [2]\n",
    "\n",
    "while state != 5:\n",
    "    next_step_index = np.argmax(Q[state, :])\n",
    "    steps.append(next_step_index)\n",
    "    state = next_step_index\n",
    "\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent has followed one of optimal paths according to the given schema of the building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
